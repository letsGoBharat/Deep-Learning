{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def LoadBatch(filename):\n",
    "    \"\"\" Copied from the dataset website \"\"\"\n",
    "    import pickle\n",
    "    with open('datasets/'+filename, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1') #encoding='latin1' is required for unpickling NumPy arrays\n",
    "    return dict\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: a list of labels\n",
    "        return\n",
    "            - one hot encoding matrix (number of labels, number of class)\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((len(x), 10))\n",
    "    \n",
    "    for idx, val in enumerate(x):\n",
    "        encoded[idx][val] = 1\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "def normalize(x, mean, std):\n",
    "    return (x-mean)/std\n",
    "\n",
    "def EvaluateClassifier(X, W, b):\n",
    "    s = (W @ X.T) + b\n",
    "    p = softmax(s)    \n",
    "    return p\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" Standard definition of code \"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def ComputeCost(X,Y,W,b,lamda):\n",
    "    p = EvaluateClassifier(X, W, b)\n",
    "    lcross = -Y.T*np.log(p)\n",
    "    cost = (1/X.shape[0]) * (np.sum(lcross) + (lamda * np.sum(W**2)))\n",
    "    return cost\n",
    "\n",
    "def ComputeAccuracy(X, Y, W, b):\n",
    "    p = EvaluateClassifier(X, W, b)\n",
    "    max_idx = np.argmax(p, axis=0)\n",
    "    acc = np.count_nonzero((Y - max_idx)==0)/X.shape[0]\n",
    "    return acc\n",
    "\n",
    "def ComputeGradients(X, Y, W, b, lamda):\n",
    "    P = EvaluateClassifier(X, W, b)\n",
    "    grad_batch = -(Y.T - P)\n",
    "    grad_W = 1/X.shape[0]*(grad_batch @ X) + 2*lamda*W\n",
    "    grad_b = np.reshape(1/X.shape[0]*(grad_batch @ np.ones(X.shape[0])), (Y.shape[1], 1))\n",
    "    return [grad_W, grad_b]\n",
    "    \n",
    "def ComputeGradsNum(X, Y, W, b, lamda, h):\n",
    "    \"\"\" Converted from matlab code \"\"\"\n",
    "    no = W.shape[0]\n",
    "    d = X.shape[0]\n",
    "\n",
    "    grad_W = np.zeros(W.shape);\n",
    "    grad_b = np.zeros((no, 1));\n",
    "\n",
    "    c = ComputeCost(X, Y, W, b, lamda);\n",
    "\n",
    "    for i in range(len(b)):\n",
    "        print(i)\n",
    "        b_try = np.array(b)\n",
    "        b_try[i] += h\n",
    "        c2 = ComputeCost(X, Y, W, b_try, lamda)\n",
    "        grad_b[i] = (c2-c) / h\n",
    "\n",
    "    for i in range(W.shape[0]):\n",
    "        print(i)\n",
    "        for j in range(W.shape[1]):\n",
    "            W_try = W\n",
    "            W_try[i,j] += h\n",
    "            c2 = ComputeCost(X, Y, W_try, b, lamda)\n",
    "            grad_W[i,j] = (c2-c) / h\n",
    "\n",
    "    return [grad_W, grad_b]\n",
    "\n",
    "def ComputeGradsNumSlow(X, Y, W, b, lamda, h):\n",
    "    \"\"\" Converted from matlab code \"\"\"\n",
    "    no = W.shape[0]\n",
    "    d = X.shape[0]\n",
    "\n",
    "    grad_W = np.zeros(W.shape);\n",
    "    grad_b = np.zeros((no, 1));\n",
    "\n",
    "    for i in range(len(b)):\n",
    "        b_try = np.array(b)\n",
    "        b_try[i] -= h\n",
    "        c1 = ComputeCost(X, Y, W, b_try, lamda)\n",
    "\n",
    "        b_try = np.array(b)\n",
    "        b_try[i] += h\n",
    "        c2 = ComputeCost(X, Y, W, b_try, lamda)\n",
    "\n",
    "        grad_b[i] = (c2-c1) / (2*h)\n",
    "\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_try = np.array(W)\n",
    "            W_try[i,j] -= h\n",
    "            c1 = ComputeCost(X, Y, W_try, b, lamda)\n",
    "\n",
    "            W_try = np.array(W)\n",
    "            W_try[i,j] += h\n",
    "            c2 = ComputeCost(X, Y, W_try, b, lamda)\n",
    "\n",
    "            grad_W[i,j] = (c2-c1) / (2*h)\n",
    "\n",
    "    return [grad_W, grad_b]\n",
    "\n",
    "def montage(W):\n",
    "    \"\"\" Display the image for each label in W \"\"\"\n",
    "    fig, ax = plt.subplots(2,5)\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            im = W [5 * i + j,:].reshape (32,32,3, order = 'F')\n",
    "            sim = (im-np.min(im[:]))/(np.max(im[:])-np.min(im[:]))\n",
    "            sim = sim.transpose(1,0,2)\n",
    "            ax[i][j].imshow(sim, interpolation='nearest')\n",
    "            ax[i][j].set_title(\"y=\"+str(5*i+j))\n",
    "            ax[i][j].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def mini_batch_gd(n, n_batch, X_train, Y_train, X_train_val, Y_train_val, Y_trn, Y_val, eta, n_epochs, W, b, lamda):\n",
    "    w_star = W\n",
    "    b_star = b\n",
    "    cost_train = []\n",
    "    cost_val = []\n",
    "    accuracy_train = []\n",
    "    accuracy_val = []\n",
    "    for epoch in range(n_epochs):\n",
    "        if (epoch+1)%10 == 0:\n",
    "                print('Epoch' + str(epoch+1))\n",
    "        X_batch = np.zeros((n_batch, X_train.shape[1]))\n",
    "        Y_batch = np.zeros((n_batch, Y_train.shape[1]))\n",
    "        for j in range(1, int(n/n_batch)):\n",
    "            start_idx = (j)*n_batch\n",
    "            end_idx = (j+1)*n_batch\n",
    "            X_batch = X_train[start_idx:end_idx, :]\n",
    "            Y_batch = Y_train[start_idx:end_idx, :]\n",
    "            X_batch_val = X_train_val[start_idx:end_idx, :]\n",
    "            Y_batch_val = Y_train_val[start_idx:end_idx, :]\n",
    "            Y_trn_batch = Y_trn[start_idx:end_idx]\n",
    "            Y_val_batch = Y_val[start_idx:end_idx]\n",
    "            grad_w, grad_b = ComputeGradients(X_batch, Y_batch, w_star, b_star, lamda)\n",
    "            w_star -= eta * grad_w\n",
    "            b_star -= eta * grad_b\n",
    "        cost_train.append(ComputeCost(X_batch, Y_batch, w_star, b_star, lamda))\n",
    "        cost_val.append(ComputeCost(X_batch_val, Y_batch_val, w_star, b_star, lamda))\n",
    "        accuracy_train.append(ComputeAccuracy(X_batch, Y_trn_batch, w_star, b_star))\n",
    "        accuracy_val.append(ComputeAccuracy(X_batch_val, Y_val_batch, w_star, b_star))\n",
    "    plot_performance(cost_train, cost_val, n_epochs, 'cost')\n",
    "    plot_performance(accuracy_train, accuracy_val, n_epochs, 'acc')\n",
    "    return w_star, b_star\n",
    "\n",
    "def plot_performance(train_data, val_data, n_epochs, plot_type):\n",
    "    if plot_type == 'cost':\n",
    "        plt.title('Cost Fucntion')\n",
    "    elif plot_type == 'acc':\n",
    "        plt.title('Accuracy')\n",
    "    plt.xlim(0, n_epochs)\n",
    "    plt.plot(train_data, label = \"Training data\")\n",
    "    plt.plot(val_data, label = \"Validation data\")\n",
    "    plt.legend()\n",
    "    #plt.savefig('result_pics/' + plot_type + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch10\n",
      "Epoch20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-53562cd1f995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#Training and Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m w_star, b_star = mini_batch_gd(total_samples, n_batch, img_trn_nrm, Y_train_one_hot, img_val_nrm, Y_val_one_hot,\n\u001b[0;32m---> 40\u001b[0;31m                                           Y_train, Y_val, eta, n_epochs, W, b, lamda)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;31m#show image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mmontage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c2cb13a8ea4a>\u001b[0m in \u001b[0;36mmini_batch_gd\u001b[0;34m(n, n_batch, X_train, Y_train, X_train_val, Y_train_val, Y_trn, Y_val, eta, n_epochs, W, b, lamda)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mY_trn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_trn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mY_val_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mgrad_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComputeGradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mw_star\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mb_star\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c2cb13a8ea4a>\u001b[0m in \u001b[0;36mComputeGradients\u001b[0;34m(X, Y, W, b, lamda)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluateClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgrad_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mgrad_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_batch\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlamda\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_batch\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgrad_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#load image data \n",
    "image_data_training = LoadBatch('data_batch_1')\n",
    "image_data_validation = LoadBatch('data_batch_2')\n",
    "image_data_test = LoadBatch('test_batch')\n",
    "\n",
    "#Training data\n",
    "X_train = image_data_training['data']\n",
    "Y_train_one_hot = one_hot_encode(image_data_training['labels'])\n",
    "Y_train = image_data_training['labels']\n",
    "\n",
    "#Validation data\n",
    "X_val = image_data_validation['data']\n",
    "Y_val_one_hot = one_hot_encode(image_data_validation['labels'])\n",
    "Y_val = image_data_validation['labels']\n",
    "\n",
    "#Test data\n",
    "X_test = image_data_test['data']\n",
    "Y_test = image_data_test['labels']\n",
    "\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "#Normalized data\n",
    "img_trn_nrm = normalize(X_train, mean, std)\n",
    "img_val_nrm = normalize(X_val, mean, std)\n",
    "img_test_nrm = normalize(X_test, mean, std)\n",
    "\n",
    "\n",
    "W = np.random.normal(0, 0.01, (10, img_val_nrm.shape[1]))\n",
    "b = np.random.normal(0, 0.01, (10, 1))\n",
    "\n",
    "n_batch = 100\n",
    "eta = 0.1\n",
    "n_epochs = 40\n",
    "lamda = 0\n",
    "total_samples = img_trn_nrm.shape[0]\n",
    "\n",
    "#Training and Validation\n",
    "w_star, b_star = mini_batch_gd(total_samples, n_batch, img_trn_nrm, Y_train_one_hot, img_val_nrm, Y_val_one_hot,\n",
    "                                          Y_train, Y_val, eta, n_epochs, W, b, lamda)\n",
    "#show image\n",
    "montage(w_star)\n",
    "\n",
    "#Accuracy\n",
    "train_acc = ComputeAccuracy(img_trn_nrm, Y_train, w_star, b_star)\n",
    "print('Accuracy on training data: ' + str(train_acc))\n",
    "val_acc = ComputeAccuracy(img_val_nrm, Y_val, w_star, b_star)\n",
    "print('Accuracy on validation data: ' + str(val_acc))\n",
    "test_acc = ComputeAccuracy(img_test_nrm, Y_test, w_star, b_star)\n",
    "print('Accuracy on test data: ' + str(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.095444142164894e-05\n",
      "5.684341914685075e-08\n"
     ]
    }
   ],
   "source": [
    "#Compare gradients\n",
    "reg = 1\n",
    "grad_w_fast, grab_b_fast = ComputeGradients(img_trn_nrm[0:1, :], Y_train_one_hot[0:1, :], W, b, reg)\n",
    "grad_w_slow, grab_b_slow = ComputeGradsNumSlow(img_trn_nrm[0:1, :], Y_train_one_hot[0:1, :], W, b, reg, 1e-6)\n",
    "diff_w = np.abs(np.sum(grad_w_fast) - np.sum(grad_w_slow))\n",
    "diff_b = np.abs(np.sum(grab_b_fast) - np.sum(grab_b_slow))\n",
    "print(diff_w)\n",
    "print(diff_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
